# 深度估计：基础和直觉

翻译自[towarddatascience]()


![](https://miro.medium.com/max/700/1*T2HqDxVSXO3xLjm3U-zkkA.jpeg)

照片由Shea Rouda在Unsplash上拍摄

## 深度对于 3D 视觉至关重要

测量相对于相机的距离仍然很困难，但绝对是开启自动驾驶、3D 场景重建和 AR 等激动人心的应用的关键。在机器人技术中，深度是执行感知、导航和规划等多项任务的关键先决条件。

创建 3D 地图将是另一个有趣的应用程序，计算深度允许我们将从多个视图捕获的[图像支持投影到 3D 中。](https://towardsdatascience.com/inverse-projection-transformation-c866ccedef1c)然后，所有点的配准和匹配可以完美地重构场景。

![](https://miro.medium.com/max/700/0*PuFkan_sqa1xUzmD.jpg)

来源：3D 重建场景

需要解决的一些具有挑战性的问题包括 **对应匹配，** 由于纹理、遮挡、非朗伯表面等原因，这可能很困难， **解决模糊解决方案，** 其中许多 3D 场景实际上可以在图像平面上给出相同的图片即预测深度不是唯一的。

从相机中检索距离信息因其相对便宜的生产成本和密集的表示而非常诱人。将这项技术推广到大众就像拥有一个已经很容易获得的相机一样容易。目前，检索深度的最佳替代方法是使用激光雷达等主动距离传感器。它们是天然的高保真传感器，可提供高度精确的深度信息。

在从事深度估计工作后，尤其是在自动驾驶汽车的应用中，由于遮挡、场景中的动态对象和不完美的立体对应等各种原因，它确实具有挑战性。对于立体匹配算法，反光、透明、镜面是最大的敌人。例如，汽车的挡风玻璃通常会降低匹配并因此降低估计。因此，大多数公司仍然依靠激光雷达来可靠地提取距离。然而，自动驾驶汽车感知堆栈的当前趋势正在转向传感器融合，因为[每个传感器](https://medium.com/swlh/camera-lidar-projection-navigating-between-2d-and-3d-911c78167a94)在它们提取的特征中都有其优势。尽管如此，自深度学习问世以来，这一领域已经获得了很大的关注并取得了显著成果。许多研究致力于解决这些问题。

在计算机视觉中，深度是从两种流行的方法中提取的。即， **来自单目图像** （静态或连续）的**深度或来自立体图像**的深度（通过利用极线几何）。这篇文章将重点介绍深度估计的背景以及与之相关的问题。需要对相机投影几何有充分的了解才能完成。

通过阅读这篇文章，我希望您对一般的深度感知有一个直观的了解。此外，概述了深度估计研究的趋势和方向。然后，我们将讨论一些（许多）相关问题。

各种深度估计算法将在后续文章中详细阐述，以免细节过多！

# 我们如何看待世界

![](https://miro.medium.com/max/556/1*RhCcQ6hlpAe5JuM5nNqvUw.png)

图 2. 投射到视网膜上（左）。投影到图像平面（右）

让我们从我们人类一般如何感知深度开始。这将为我们提供一些关于深度估计的有价值的见解，因为其中许多方法都来自我们的人类视觉系统。机器和人类视觉在图像形成方式上都有相似之处（图 2）。从理论上讲，当来自光源的光线照射到表面时，它会反射并指向我们视网膜的背面，将它们投射出来，我们的眼睛会将它们处理为 2D [1]，就像在图像平面上形成图像一样。

那么，当投影场景是 2D 时，我们如何实际测量距离并了解 3D 环境呢？例如，假设有人要给你一拳，你本能地知道你什么时候会被击中，当他/她的拳头靠得太近时，你会本能地躲开！或者，当您开车时，您可以通过某种方式判断何时踩油门或踩刹车，以便与许多其他驾驶员和行人保持安全距离。

=这里的工作机制是我们的大脑开始通过识别场景的大小、纹理和运动等模式来推理传入的视觉信号= =**深度线索**=  =.= 没有关于图像的距离信息，但不知何故我们可以毫不费力地解释和恢复深度信息。我们感知场景的哪个方面离我们很近和离我们更远。此外，这些线索使我们能够将平面图像上的物体和表面视为 3D [1]。

## 如何破坏深度（不是人类/计算机视觉）

只是为了突出一个有趣的事实，解释这些深度线索从如何将场景投影到人类和相机视觉中的 **透视图开始。** 另一方面，正视图或侧视图的**正投影是破坏所有深度信息的投影。**

考虑图 3，观察者可以解开房子的哪个方面更靠近他/她，如左图所示。然而，从正确的图像中区分相对距离是完全不可能的。甚至背景也可能与房子位于同一平面上。

![](https://miro.medium.com/max/700/1*aAdZtXlDs4n3ZLt7B2FJ4Q.png)

图 3. 透视投影（左）。正交投影（右）

# 使用线索判断深度

深度线索基本上有 4 类：静态单目、运动深度、双目和生理线索 [2]。我们下意识地利用这些信号来非常好地感知深度。

## 图形深度提示

我们从单个静止图像中感知深度的能力取决于场景中事物的空间排列。下面，我总结了一些提示，使我们能够推断不同物体的距离。从您与地球母亲的日常互动中，您可能已经觉得它很自然。希望不要过多考虑找出各种线索。

![](https://miro.medium.com/max/700/1*b9m4cbG4P8Jgo0Wxrjbnfg.jpeg)

照片由Mateus Campos Felipe在Unsplash上拍摄

![](https://miro.medium.com/max/700/1*EmjAoOiag9YvCGys6ZNdAw.png)

加州大学伯克利分校进行了一项[有趣的研究，他们通过实验表明，当地平线可见时，我们有一种压倒性的趋势，即利用这一特性来快速感知深度。](https://www.researchgate.net/publication/41413971_Vertical_position_as_a_cue_to_pictorial_depth_Height_in_the_picture_plane_versus_distance_to_the_horizon)当您查看上面的图片时，这对您来说是真的吗？

## 来自运动的深度提示（运动视差）

![](https://miro.medium.com/max/600/1*rMo1QZP5FwCKTDedH2dJKw.gif)

图 5.运动视差

这对您来说也不足为奇。作为观察者，当你在运动时，你周围的事物会比远处的事物更快地经过。东西出现得越远，它似乎离观察者越慢。

## 立体视觉的深度线索（双目视差）

 **视网膜视差** ：另一个有趣的事件使我们能够识别深度，可以通过一个简单的实验直观地理解。

![](https://miro.medium.com/max/350/1*2jaUAl-M9p8SmKFk2C2LyA.jpeg)

图 6.来源

将你的食指放在你面前，尽可能靠近你的脸，闭上一只眼睛。现在，反复关闭一个并打开另一个。观察到你的手指在移动！左右眼观察到的视野差异称为 **视网膜视差** 。现在伸出你的手指在手臂的长度并执行相同的动作。您应该注意到手指位置的变化变得不那么明显了。这应该会给你一些关于立体视觉如何工作的线索。

这种现象被称为[**立体视觉**](https://en.wikipedia.org/wiki/Stereopsis) **；** 由于 2 种不同的世界视角而具有感知深度的能力。通过比较两只眼睛的视网膜图像，大脑计算距离。差距越大，事物离你越近。

# 计算机视觉中的深度估计

深度估计的目标是获得场景空间结构的表示，恢复图像中物体的三维形状和外观。这也被称为逆问题[3]，在没有足够的信息来完全指定解决方案的情况下，我们试图恢复一些未知数。这意味着 2D 视图和 3D 之间的映射不是唯一的（图 12）我将在本节中介绍经典立体方法和深度学习方法。

那么机器实际上是如何感知深度的呢？我们可以以某种方式转移上面讨论的一些想法吗？最早具有令人印象深刻结果的算法始于 90 年代使用立体视觉的深度估计。密集立体对应算法 [4] [5] [6] 取得了很大进展。研究人员能够利用几何学在数学上约束和复制立体视觉的概念，同时实时运行。本文总结了所有这些想法[7]。

至于单目深度估计，它最近开始通过使用神经网络学习直接提取深度的表示来获得普及[8]。其中深度线索是通过基于梯度的方法隐式学习的。除此之外，自监督深度估计[9][10][11]也取得了很大进步。这是特别令人兴奋和开创性的！在这种方法中，通过优化代理信号来训练模型来预测深度。训练过程中不需要地面实况标签。大多数研究要么利用多视图几何或核几何等几何线索来学习深度。我们稍后会谈到这一点。

## 立体视觉的深度估计

![](https://miro.medium.com/max/700/1*_pfJ7d-zojcNPMQl7P7qOQ.png)

图 7. 对极几何（左）。校正后的图像（右）

使用立体相机求解深度的主要思想涉及**三角测量**和**立体匹配**的概念。形式依赖于良好的校准和**校正**来约束问题，以便它可以在称为核平面的 2D 平面上建模 **，** 这大大减少了后者沿**核线**的线搜索（图 7） **。** 有关极线几何的更多技术细节将在以后的帖子中讨论。

类似于双目视差，一旦我们能够匹配两个视图之间的像素对应关系，下一个任务就是获得对差异进行编码的表示。这种表示称为 **视差，d。** 为了从视差中获得深度，可以从相似的三角形中得出公式（图 8）

![](https://miro.medium.com/max/700/1*bkvPygXXdiDWaWUGfwmhWw.png)

图 8. 立体几何

步骤如下

* 从特征描述符中识别相似点。
* 使用匹配成本函数匹配特征对应。
* 使用对极几何，在一个相框中找到对应关系并将其与另一个相匹配。匹配成本函数 [6] 用于测量像素相异性
* 计算已知对应的差异 `d = x1 — x2`，如图 8 所示。
* 根据已知视差计算深度 `z = (f*b)/d`

![](https://miro.medium.com/max/452/1*2YIDdj-r9MK_LkdyXKzq9A.png)

图 9. Kitti 的视差表示

## 深度学习时代

深度学习擅长识别、检测和场景理解等高级感知和认知任务。深度感知属于这一类，同样应该是一种自然的前进方式。目前有 3 个广泛的框架来学习深度：

 **监督学习** ：直接从单目图像估计深度的开创性工作始于 Saxena [8]。他们学会了通过监督学习直接从 2D 图像中的单眼线索回归深度，通过最小化回归损失。从那时起，人们提出了多种方法来通过提出新的架构或损失函数来改进表示学习

 **使用 SFM 框架进行自我监督深度估计：** 该方法将问题描述为学习从视频序列中生成新视图。神经网络的任务是从源视图生成目标视图，方法 `I_t`是在不同的时间步拍摄图像 `I_t-1, I_t+1`并应用从姿势网络学习的变换来执行图像变形。通过使用空间变换网络 [14] 以可微的方式将扭曲视图合成视为监督，使训练成为可能。在推理时，深度 CNN 将从单视图 RGB 图像中预测深度（图 10）。我建议您阅读本文以了解更多信息。请注意，此方法确实存在一些缺点，例如无法确定比例和对下一节中描述的移动对象进行建模。

![](https://miro.medium.com/max/700/1*1FBKoFDTM9QP579P_q_CPw.png)

图 10.无监督单目深度估计

 **使用 Stereo 进行自我监督单目深度估计** ：另一种有趣的方法。在这里（图 11），模型不会将图像序列作为输入，而是 `d_l, d_r`仅从左侧 RGB预测视差 `I_l`。与上述方法类似，空间变换器网络 `I_l, I_r`使用视差扭曲 RGB 图像对。回想一下 `x2 = x1 — d`。因此可以合成配对视图，并使用重建视图 `I_pred_l, I_pred_r`和目标视图之间的重建损失 `I_l, I_r`来监督训练。

为了使这种方法起作用，假设基线必须是水平的并且是已知的。必须校正图像对以使通过视差的变换准确。这样计算 `d = x1 — x2`如图 8 所示。

![](https://miro.medium.com/max/250/1*3zRJ0bCE2uvQ9AlHruEHxw.png)

图 11.使用立体的自监督单目深度估计

## CNN 深度线索和有偏见的学习

理解和破译黑匣子一直是可解释机器学习的持续研究。在深度估计的背景下，一些工作已经开始研究神经网络依赖于哪些深度线索或从特定数据集中学习的归纳偏差。

在[Tom 等人的开创性工作 ICCV 2019](https://openaccess.thecvf.com/content_ICCV_2019/papers/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.pdf)中，他们执行了几个简单的测试来通过实验找到估计深度与场景结构之间的关系。请注意，这是在 Kitti 数据集上完成的，该数据集是具有固定摄像机位置的室外道路场景，并且具有一些可见的消失点和地平线。

![](https://miro.medium.com/max/700/1*bQlhTNOnzRn-NzgS9SS0ew.png)

来源

 **物体相对于地面接触点的位置提供了强大的上下文信息** ：对于道路上的物体，通过增加汽车在垂直方向上的位置。我们看到，当模型离地面较远时，它无法很好地估计深度。

![](https://miro.medium.com/max/700/1*kPvqxtGURYE-s7a2MhLntw.png)

![](https://miro.medium.com/max/700/1*wsUCj0hyMd7O5pbmTrS1dQ.png)

来源：物体下方的阴影作为深度估计的强大特征

 **形状不重要，但阴影重要** ：在另一个实验中，通过放置带有人工投射阴影的任意对象，模型可以合理地估计深度，即使它在训练期间不可用。

[Rene 等人在 TPAMI 2020](https://arxiv.org/pdf/1907.01341.pdf)上完成了有趣的工作，他们在由室内和室外场景组成的相对大规模数据集上训练深度模型。从观察来看，存在一种自然偏差，即图像的下部总是靠近相机。这可以被视为下图中右上角示例中的故障模式。此外，深度模型倾向于预测内容，而不是在左下角的情况下识别为镜子中的反射。论文中还可以找到许多其他有趣的发现。

![](https://miro.medium.com/max/700/1*lh8ke4MSvYrkt8ZIYuYKDQ.png)

来源

所进行的研究仍然相当有限。需要做很多工作才能提供更确凿的调查结果。

# 为什么测量深度如此困难？

最后，让我们尝试理解深度估计的一些基本问题。罪魁祸首在于将 3D 视图投影到丢失深度信息的 2D 图像。当有运动和移动的物体时，另一个问题是根深蒂固的。我们将在本节中介绍它们。

## 深度估计是不适定的

通常在进行单目深度估计研究时，许多作者会提到从单个 RGB 图像估计深度的问题是一个不适定的逆问题。这意味着世界上观察到的许多 3D 场景确实可以对应同一个 2D 平面（图 11 和 12）。

![](https://miro.medium.com/max/700/1*WqJTNjq6sirSmtxA_0tcKA.png)

图 12.来源

## 不适定：单目深度估计的尺度模糊

回想一下，调整焦距将按比例缩放图像平面上的点。现在，假设我们将整个场景 X 缩放某个因子 `k`，同时将相机矩阵 P 缩放 因子 `1/k`，图像中场景点的投影保持完全相同

`x = PX = (1/k)P * (kX) = x`

![](https://miro.medium.com/max/700/1*IzEBZdx8r9eLW8G7dBd9Zg.png)

也就是说，我们永远无法仅从图像中恢复实际场景的确切比例！

请注意，单目基础技术存在此问题，因为可以为具有已知基线的立体装置恢复比例。

## 病态：投影歧义

假设我们对场景进行几何变换，有可能变换后这些点会映射到平面上的同一个位置。再一次给我们留下同样的困难。见下图

![](https://miro.medium.com/max/700/1*j_zfv3eyqSUxEoygAszd3A.png)

图 13. 变换后的对象投影映射到平面中的同一点

## 降低匹配的属性

对于需要三角测量的基于立体或多视图的深度估计，它通常涉及[**Detect-Describe-Match**](https://www.researchgate.net/publication/292995470_Image_Features_Detection_Description_and_Matching)的管道。当场景是从极其不同的视点或图像之间的光照变化不同时，匹配变得非常困难。下图给出了一个极端情况，其中描述符无法区分特征。这些有问题的案例包括：

* 无纹理区域：许多像素将具有相同的像素强度
* 反光面
* 重复模式
* 遮挡：对象在一个视图中被遮挡，但在另一个视图中不被遮挡。
* 违反[朗伯](https://en.wikipedia.org/wiki/Lambert%27s_cosine_law)属性：朗伯曲面是指无论从何处观察，看起来都具有相同亮度的曲面。当图像从 2 个不同视图显示相同场景时，由于非理想漫反射，相应的亮度强度可能不相等。

![](https://miro.medium.com/max/700/1*Pp_3gGOF7ivm75nO2s4N2A.png)

一个非常困难的场景，具有很大的视野差异和不同的人群。

## 移动物体违反了 SFM 方法的静态假设

场景中的动态对象进一步使估计过程复杂化。通过运动结构进行深度估计涉及移动相机和连续的静态场景。 **这个假设必须适用于匹配和对齐像素** 。当场景中有移动的物体时，这个假设就会失效。为此，许多研究人员已经研究了几种方法来模拟场景中的移动物体，方法是使用光流 [12] 结合速度信息，或者使用实例分割掩码来模拟物体从一帧到另一帧的运动 [13]。

# 下一步是什么

我希望您从这篇介绍性文章中获得了一些关于深度估计的有价值的见解，为什么它是一项具有挑战性但极其重要的任务，以及该技术的当前状态。我坚信可以使用相机和视觉来解决深度问题。并且对此持乐观态度。因为我们自己只依靠单眼或双目视觉与我们的环境互动。

> 感谢您阅读本文。希望它能给你一些好的见解！关注以查看更多关于计算机视觉和机器学习的帖子。随时留下任何反馈:)

[深度估计的不确定性你确定你的预测深度吗？向datascience.com](https://towardsdatascience.com/uncertainty-in-depth-estimation-c3f04f44f9)

[自监督深度估计：分解想法在这篇文章中，我想通过自我监督学习来分解深度估计的多种想法。为了…向datascience.com](https://towardsdatascience.com/self-supervised-depth-estimation-breaking-down-the-ideas-f212e4f05ffa)

[逆投影变换深度和反投影向datascience.com](https://towardsdatascience.com/inverse-projection-transformation-c866ccedef1c)

# 参考


[1] [Introduction to the Science of Vision](http://www.mind.ilstu.edu/curriculum/vision_science_intro/vision_science_intro.php), David L. Anderson

[2] Visual Comfort of Binocular and 3-D Displays, Frank L. Kooi, Alexander Toet, *in* [Proceedings of SPIE — The International Society for Optical Engineering](https://www.researchgate.net/journal/0277-786X_Proceedings_of_SPIE-The_International_Society_for_Optical_Engineering) 25(2):99–108 · August 2004

[3] Computer Vision: Algorithms and Applications, Richard Szeliski

[4] Okutomi, M. and Kanade, T. (1993). A multiple baseline stereo.  *IEEE Transactions on Pattern Analysis and Machine Intelligence* , 15(4):353–363.

[5] Boykov, Y., Veksler, O., and Zabih, R. (1998). A variable window approach to early vision.  *IEEE Transactions on Pattern Analysis and Machine Intelligence* , 20(12):1283–1294.

[6] Birchfield, S. and Tomasi, C. (1999). Depth discontinuities by pixel-to-pixel stereo.  *International Journal of Computer Vision* , 35(3):269–293.

[7] Scharstein, D. and Szeliski, R. (2002). A taxonomy and evaluation of dense two-frame stereo correspondence algorithms.  *International Journal of Computer Vision* , 47(1):7– 42.

[8] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. NIPS, 2014.

[9] R. Garg, G. Carneiro, and I. Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. ECCV, 2016.

[10] T. Zhou, M. Brown, N. Snavely, and D. Lowe. Unsupervised learning of depth and ego-motion from video. CVPR, 2017.

[11] C. Godard, O. M. Aodha, and G. J. Brostow. Unsupervised monocular depth estimation with left-right consistency. CVPR, 2017.

[12] Z. Yang, P. Wang, Y. Wang, W. Xu, and R. Nevatia. Every pixel counts: Unsupervised geometry learning with holistic 3d motion understanding. arxiv.org/pdf/1806.10556, 2018.

[13] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5667–5675, 2018.

[14] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. Spatial transformer networks. In NIPS, 2015.
